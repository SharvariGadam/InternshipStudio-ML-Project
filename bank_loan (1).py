# -*- coding: utf-8 -*-
"""bank_loan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tw8BCscJyNagMsAkzFyPo2m8rNgcJSPz

#             **Project Title**

#**Marketing Campaign for Banking Products**

By Sharvari Gadam
(gadam.sharvari16@gmail.com)

Objective:
*   The classification goal is to predict the likelihood of a liability customer buying personal
loans.

*   To sell more Personal Loan to the Bank Customers.
*   To identify the potential customers to have higher probability of purchasing the loan

**Import libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

#the csv file is uploaded
from google.colab import files
uploaded = files.upload()

import io
dataset = pd.read_csv(io.BytesIO(uploaded['Bank_Personal_Loan_Modelling.csv']))

#to display top 6 data
dataset.head(6)

#to display last 5 data
dataset.tail(5)

#to display datatype of each column and also display the not null rows for each column
dataset.info()

# to display the summary stats which include count,mean,std,min,max,etc
dataset.describe()

#display how many null values are present in the dataset
dataset.isnull().sum()

"""# **Dropping Irrelevant columns**"""

experience = dataset['Experience']
age = dataset['Age']
correlation = experience.corr(age)
correlation

"""After Checking Correlation Between Age and Experience which occured as most same so have dropped the experience column"""

dataset = dataset.drop(['ID','Experience'],axis=1)
dataset.head(5)

"""**Point 3:**

**3.1  Number of unique in each column.**
"""

#Number of Unique values in the columns
dataset.nunique()

"""Zip code has 467 distinct values.It is a nominal variable which has too many levels .Its better to drop zipcode as well"""

dataset.drop('ZIP Code' , axis = 1)

"""**3.2 Number of people with zero mortgage**"""

#dataset.Mortgage.count(0)
#print(len(dataset.Mortgage))
z=dataset[['Mortgage']].count()
z

#(dataset.Mortgage ==0).sum()
zero_mortgage = len(dataset.Mortgage) - np.count_nonzero(dataset.Mortgage)
zero_mortgage

"""**3.3 Number of people with zero credit card spending per month?**"""

len(dataset.CreditCard)

zero_creditcard = len(dataset.CCAvg) - np.count_nonzero(dataset.CCAvg)
zero_creditcard

"""Here , only 106 peoples with zero Credit card spend per month

**3.4 Value counts of all categorical columns.**

**FAMILY**
"""

dataset.Family.value_counts()

#Viewing Pie Chart of this distribution
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label = ['1', '2','3','4']
ax.pie(dataset['Family'].value_counts(),labels=label,colors=['lightskyblue','gold','red','green'],autopct='%1.1f%%',explode=(0.1,0,0,0),shadow=True,radius=1.2)
plt.title("Family size")
plt.legend()
plt.show()

#Viewing the bar graph representation of this distribution
label = ['1','2','3','4']
plt.bar(label,dataset['Family'].value_counts(),width=0.7,color=['green','blue'])
plt.xlabel('Family Size')
plt.show()

"""Thus, from both the pie chart and bar graph we can say that family of person with size '1' has highest percentage i.e 29.4%  and the following sizes are 2(25.9%),3(24.4%),4(20.2%)

**AGE**
"""

#Count of each type of unique values in columns
plt.plot(dataset['Age'].value_counts())
plt.xlabel('Age')
plt.show()

"""**EDUCATION**"""

dataset.Education.value_counts()

#Viewing the bar representation of this distribution
label=['1:undergraduate','2:Graduate','3:Advance/Professional']
plt.bar(label,dataset['Education'].value_counts())
plt.xlabel('Education level')
plt.ylabel("No. of people")
plt.show()

#Viewing Pie Chart of this distribution
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label=['1:undergraduate','2:Graduate','3:Advance/Professional']
ax.pie(dataset['Education'].value_counts(),labels=label,colors=['lightskyblue','gold','red'],autopct='%1.1f%%',explode=(0.1,0,0,),shadow=True,radius=1.2)
plt.title("Education level")
plt.legend()
plt.show()

"""Thus, from both the pie chart and bar graph we can say that Education of person who is undergraduated(41.9%) is more than graduate(30.0%) and advance/professional(28.1%)

**SECURITIES ACCOUNT**
"""

dataset['Securities Account'].value_counts()

#Viewing Pie Chart of this distribution
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label = ['0: No', '1: Yes']
ax.pie(dataset['Securities Account'].value_counts(),labels=label,autopct='%1.2f%%',explode=(0,0.1))
plt.show()

"""So, only 10.44% of our customers have security accounts.

**CD ACCOUNT**
"""

dataset['CD Account'].value_counts()

#Viewing Pie Chart of this distribution
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label = ['0: No', '1: Yes']
ax.pie(dataset['CD Account'].value_counts(),labels=label,colors=['lightskyblue','gold'],autopct='%1.1f%%',explode=(0.3, 0),shadow=True,radius=1.2)
plt.title("CD Account")
plt.legend()
plt.show()

"""**ONLINE**"""

dataset.Online.value_counts()

#Viewing the bar chart representation of this data
label = ['0:Yes','1:No']
plt.bar(label, dataset['Online'].value_counts(),color=['lightskyblue','gold'])
plt.xlabel('Online facility')
plt.ylabel('No. of people')
plt.show()

#Viewing Pie Chart of this distribution
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label = ['0: No', '1: Yes']
ax.pie(dataset['Online'].value_counts(),labels=label,colors=['lightskyblue','red'],autopct='%1.1f%%',explode=(0.1, 0),shadow=True,radius=1.2)
plt.title("Online account")
plt.legend()
plt.show()

"""**CREDIT CARD**"""

dataset.CreditCard.value_counts()

#Viewing Bar Chart Representation of this data
label = ['0:Yes','1:No']
plt.bar(label, dataset['CreditCard'].value_counts(),color=['lightskyblue','green'])
plt.xlabel('Credit card by Universal Bank')
plt.ylabel('No. of people')
plt.show()

#Viewing the Pie Chart Representation of this data.
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
label = ['0:No','1:Yes']
ax.pie(dataset['CreditCard'].value_counts(),labels=label,autopct='%1.2f%%',explode=(0,0.1))
plt.show()

"""So, only 29.4% users have creditcard issued by Universal Bank"""



"""**3.4 Univariate analysis**

Analyse the single variable, with displot() gives distribution of numerical data
"""

#symetric distribution
sns.distplot(dataset.Age)

#it is right skewed plot
sns.distplot(dataset.Income)

sns.distplot(dataset.CCAvg)

sns.distplot(dataset.Education)

sns.distplot(dataset.Mortgage)

sns.distplot(dataset.Family)

sns.countplot(dataset.Family)

sns.countplot(dataset.Education)

"""**Multivariate Analysis**"""

#it seems that customers who has more income has granted the loan across education level
sns.boxplot(x='Education' , y='Income',hue='Personal Loan',data=dataset)

#majority people having securities account don't have personal loan
sns.countplot(x='Securities Account' ,hue='Personal Loan',data=dataset)

#Dropping Experience column earlier doesn't seem to be significant corelation between other variables except creditcard avg and income
#corelation matrix
fix,ax = plt.subplots(figsize=(15,10))
sns.heatmap(dataset.corr(),cmap='plasma', annot=True);

"""Dropping Experience column earlier doesn't seem to be significant corelation between other variables except creditcard avg and income
corelation matrix
"""

#plots one variable with every other variable
sns.pairplot(dataset)

"""In this distplot each feature variable is plotted with every other feature variable

**4  Apply necessary transformations for the feature variables**
"""

#data_X contaion all the variables except Personal Loan
#while data_Y contains only Personal loan on which we apply algorithms

data_X = dataset.loc[:,dataset.columns !="Personal Loan"]
data_Y = dataset[["Personal Loan"]]

"""Here Power Transformer Yeo Johnson model is used on the feature variable "Income""""

from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer(method = "yeo-johnson", standardize=False)
pt.fit(data_X["Income"].values.reshape(-1,1))
yeo = pt.transform(data_X["Income"].values.reshape(-1,1))
data_X["Income"] = pd.Series(yeo.flatten())
sns.boxplot(data_X["Income"],orient="v",palette="Set2")
#sns.distplot(yeo);

pt = PowerTransformer(method = "yeo-johnson", standardize=False)
pt.fit(data_X["CCAvg"].values.reshape(-1,1))
yeo = pt.transform(data_X["CCAvg"].values.reshape(-1,1))
sns.distplot(yeo);

data_X["Mortgage_t"] = pd.cut(data_X["Mortgage"],bins = [0,100,200,300,400,500,600,700], labels = [0,1,2,3,4,5,6],include_lowest=True)
data_X.drop("Mortgage",axis=1,inplace=True)

"""*Binning is used here on the Mortgage column because the displot of mortgage is not uniform and skewed.*"""

data_X.head()



"""**Standardization (Standard Scalar)**"""

#Standard Scaler
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler() 
data_scaled = scaler.fit_transform(data_X["CCAvg"].values.reshape(-1,1))

print(data_scaled.mean(axis=0))
print(data_scaled.std(axis=0))

print('Min values (CCAvg): ', data_scaled.min(axis=0))
print('Max values (CCAvg): ', data_scaled.max(axis=0))

"""Scaling is applied to independent variables or features of data. It helps to normalise the data within a particular range.

**Normalizing Data**
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler() 
data_scaled = scaler.fit_transform(data_X["Income"].values.reshape(-1,1))

print('mean (Income): ', data_scaled.mean(axis=0))
print('std (Income): ', data_scaled.std(axis=0))

print('Min (Income): ', data_scaled.min(axis=0))
print('Max (Income): ', data_scaled.max(axis=0))

"""Normalization of data is done here where it shows mean , standard,min, max is calculated.

**Splitting Dataset into train and test data**

The dataset is splitted into two models Test data and Train data where the splitting proportion is 70% training and 30% test data.
"""

from sklearn.model_selection import train_test_split
train_X,test_X,train_Y,test_Y = train_test_split(data_X,data_Y,test_size=0.3,stratify=data_Y,random_state=0)

train_X.shape

train_X.head()

train_Y.shape

train_Y.head()

test_X.shape

test_X.head()

test_Y.shape

test_Y.head()

"""The line test_size=0.3 suggests that the test data should be 30% of the dataset and the rest should be train data. With the outputs of the shape() functions, ywe can see that we have 1500 rows in the test data and 3500 in the training data.

**Algorithm 1: Logistic Regression**
"""

from sklearn.preprocessing import StandardScaler
sc_x_train = StandardScaler()
sc_x_train.fit_transform(train_X)

sc_x_test = StandardScaler()
sc_x_test.fit_transform(test_X)

xx_train = train_X.values
yy_train = train_Y.values

from sklearn.linear_model import LogisticRegression 
classifier = LogisticRegression(random_state = 0) 
#classifier.fit(train_X,train_Y)
classifier.fit(xx_train,yy_train.ravel())
y_logistic_test = classifier.predict(test_X)
y_logistic_train = classifier.predict(train_X)

from sklearn.metrics import confusion_matrix ,classification_report
cm = confusion_matrix(test_Y, y_logistic_test) 
print ("\nConfusion Matrix : \n", cm) 

result1 = classification_report(test_Y, x_logistic_test,zero_division=1)
print("\nClassification Report:",)
print (result1)

from sklearn.metrics import accuracy_score 
print ("\nAccuracy Test Data: ", accuracy_score(test_Y, y_logistic_test)) 
print ("Accuracy Train Data: ", accuracy_score(train_Y, y_logistic_train))

cm = confusion_matrix(test_Y,y_logistic_test)
sns.heatmap(cm,annot=True)
plt.xlabel('Predicted Values')
plt.ylabel('Original Values')
plt.show()

"""*Logistic Regression*
In this algorithm ,the accuracy of **Test data :90.4%**
while that of **Train data: 90.4%**

**Algorithm 2: K Nearset Neighbours**
"""



from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(train_X)

#scaling the data
X_ss_train = scaler.transform(train_X)
X_ss_test = scaler.transform(test_X)

#import KNN algorithm from sklearn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_ss_train, train_Y)
#result = classifier.fit(xx_train,yy_train.ravel())
print(result)

#make prediction
y_pred_knn = classifier.predict(X_ss_test)
x_pred_knn = classifier.predict(X_ss_train)
#print the confusion matrix and classification report

from sklearn.metrics import classification_report, confusion_matrix
print('\nThe confusion matrix is\n',confusion_matrix(test_Y, y_pred_knn))
print('\nThe classification report is\n',classification_report(test_Y, y_pred_knn))
 
print ("\nAccuracy Test data: ", accuracy_score(test_Y, y_pred_knn)) 
print ("Accuracy Train data: ", accuracy_score(train_Y, x_pred_knn))

cm = confusion_matrix(test_Y,y_pred_knn)
sns.heatmap(cm,annot=True)
plt.xlabel('Predicted Values')
plt.ylabel('Original Values')
plt.show()

"""In *K Neighbors Classifier* the accuracy of **Train data is:97.28%** while that of **Test data:95.93%**

This algorithm is more accurate than the Logisitic Regression Algorithm used.
So we can use KNN algorithm .Here Scaling is done before applying the algorithm which gave more accurate numbers about the model.

**Algorthim 3: Random Forest**
"""

np_train_x = train_X.values
ny_train_y = train_Y.values

from sklearn.ensemble import RandomForestClassifier
rclassifier = RandomForestClassifier(n_estimators = 400,max_depth=8,random_state=0)
rclassifier.fit(np_train_x,np_train_y.ravel())
x_random_test = rclassifier.predict(test_X)
x_random_train = rclassifier.predict(train_X)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,plot_confusion_matrix
result1 = confusion_matrix(test_Y, x_random_test)
print("Confusion Matrix :")
print(result1)

result1 = classification_report(test_Y, x_random_test)
print("\n\nClassification Report:")
print (result1)

result1 = accuracy_score(test_Y,x_random_test)
print("\nAccuracy Test Data:",result1)
result2 = accuracy_score(train_Y,x_random_train)
print("Accuracy Train Data:",result2)

cm = confusion_matrix(test_Y,x_random_test)
sns.heatmap(cm,annot=True)
plt.xlabel('Predicted Values')
plt.ylabel('Original Values')
plt.show()

"""In *Random Forest* the accuracy of **Train data is:99.48%** while that of **Test data:98.73%**

This algorithm is more accurate than the Logisitic Regression Algorithm and KNN.
So we can say that Random Forest is the best among all the three algorithms .
So , we can use this model for the future purposes .
"""



"""#**Conclusion**

**Buisness Understanding of model**

*   From this very interesting project and the analysis, we can conclude the following points:

* The Dataset provided to us had no null values.
* It had the data of 5000 customers.
* ID's of customers had nothing to do with the loan.
* The Experience and Age columns were highly correlated.
* The Experience Column had faulty data i.e., it had negative values .So we dropped it
* Maximum number of customers live alone.
* Maximum number of customers are graduates.
* 9.6% of customers took loan in the last session.
* 10.44% customers have security accounts with the bank.
* 6.04% customers have Certificate of Deposit Accounts with the bank.
* 59.68% customers use Online Facilities.
* 29.4% customers have credit cards issued by Universal Bank.
* Number of customers decrease as Income increases.
* Many customers have low average credit card spending per month.
* People with High education and High Income tend to take loan more.
* People with security accounts tend to take loan less.
* Mortgage, Income and Credit card average data is skewed highly. 
* Zipcodes had too many nominal data so it will not effect much to the prediction.
* All the models seem to work well but we were required to choose model best suitable for our case, so we compared them and on comparision, found Random
* Random Forest Classifier to be the best suited for our case. And its results seem highly promising.

So, the Project can be used in the banks for the future purpose.

Therefore ,completed ML internship Project by Internship Studio.
Thanks...!

It was great experience.
"""